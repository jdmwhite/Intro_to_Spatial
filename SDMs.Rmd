---
title: "Species Distribution Models"
output: 
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 4
    theme: paper
---

Note: thank you to Geethen Singh for an earlier version of this practical.

### Learning objectives

1. Define a calibration area
2. Process environmental covariate data
3. Process species presence data
4. Create pseudo-absence species points
5. Account for spatial autocorrelation with spatial blocks
6. Fit a random forest model
7. Estimate model accuracy
8. Determine variable importance
9. Visualise variable response curves
10. Produce a habitat suitability prediction

### Introduction

Species Distribution models (SDMs, also called habitat suitability models or ecological niche models, depending on the goal) involves the identification of areas of likely species occurrence by learning a relationship between known occurrences and environmental variables (covariates). The ‘learning’ of species-environment relationships is accomplished by machine learning algorithms. For this short course, the inner-workings of these algorithms are beyond the scope of our available time. There are many papers describing SDMs, but if you are new to the field, this [Elith & Leathwick 2009](https://www.annualreviews.org/doi/abs/10.1146/annurev.ecolsys.110308.120159?casa_token=ZCcYcz4kym4AAAAA:PPSNWmUn0wy2Ieu6nVwpzX37f-67fnkVMMxfAUk74Peu01XafOnSf-SF0VuCL24WMwDu4koUzR2wFyg) paper is a good start.

SDMs are used for a range of purposes, not limited to understanding drivers of species distributions, conservation planning and risk forecasting. This tutorial is likely to be challenging for a beginner in spatial analysis, but is designed to try to balance thoroughness with simplicity. 

Our overall goal is to produce a prediction map of the most suitable habitat for a species of interest and to better understand the environmental drivers behind its current distribution. Along the way, we will carefully process our data to make sure we have the best inputs and then provide a model evaluation procedure to determine how robust our findings are.

### Tutorial

We will be using two new-ish R packages for running species distribution models: `flexSDM` and `SDMtune`. These packages have several features, which are useful in processing and modeling our SDM data. We will then use `blockCV` to create a spatial blocking space to improve the robustness of our findings. Most of the other packages, you should be familiar with by now.

```{r, message = FALSE, warning=FALSE}
#### Install.packages ----
# install.packages('raster')
# install.packages('remotes')
# devtools::install_github("sjevelazco/flexsdm")
# Select 3 and then No
# install.packages('corrplot')
# install.packages('blockCV')
# install.packages('SDMtune')

#### Load libraries ----
library(terra) # raster manipulation
library(rnaturalearth) # country boundaries
library(tidyverse) # data manipulation and plotting
library(corrplot) # correlation plots
library(blockCV) # spatial block for cross-validation
library(sf) # vector manipulation
library(flexsdm) # SDM package
library(SDMtune) # SDM package
library(patchwork) # combine ggplots
```

#### Load in data

We will be using the Protea roupelliae locality data that we used in the **Making a map** tutorial. This data has already been processed and cleaned to remove spurious locations. 

We will download data for all countries that overlap with the species localities and then merge these together to make a single country boundary.

Lastly we will be using the WorldClim data as our environmental covariates. This can be downloaded using the `raster` package or loaded in directly from file. 

```{r}
#### Load in data ----
# Read species data 
protea <- vect("output/files/making_a_map/p_roup_gbif.shp")
# download and load SA boundary
sern_a <- ne_countries(scale = 'medium', country = c('South Africa', 'Lesotho', 'Swaziland'), returnclass = 'sf')
# dissolve to outer boundary
sern_a %>% group_by(level) %>% summarise() %>% vect() -> sern_a_dissolve
# Download bioclim data
# r <- rast(raster::getData("worldclim",var="bio",res=5))
# returns 19 variables
# or alternatively load from disk
r <- rast("data/sdm/worldclim.tif")
```

Let's plot out our species and area of interest. For some reason, a few border lines remain in our country border file - this won't impact our modeling going forward.

```{r}
#### Visualise raw data ----
plot(r[[1]])
plot(sern_a_dissolve, add = T)
plot(protea, add=T)
```

Again, we would like to exclude Marion Island, so crop the country boundary data to a reasonable extent.

```{r}
#### Edit extent of Sern A vector 
plot(sern_a_dissolve)
sern_a_dissolve <- crop(sern_a_dissolve, ext(15, 33, -35, -20))
plot(sern_a_dissolve)
```

Check and fix the projections.

```{r}
#### Check projections ----
crs(r) == crs(protea)
crs(r) == crs(sern_a_dissolve)
sern_a_dissolve <- project(sern_a_dissolve, r)
protea <- project(protea, r)
crs(r) == crs(sern_a_dissolve)
crs(r) == crs(protea)
```

#### Calibration area

It is really important to only predict our habitat suitability to a region where the species could feasibly occur. So instead of using the whole of southern Africa, we will limit this to a calibration area around our species localities within a 150km buffer zone.

```{r}
#### Calibration area ----
# Create a 150km buffer around our species points to create a prediction region
ca_protea <- calib_area(data = as.data.frame(protea),
                        x = 'lon', y = 'lat',
                        method = c('buffer', width = 150000),
                        crs = crs(protea))

# Plot it out to see if this works
par(mfrow = c(1,1))
plot(ca_protea)
plot(protea, add = TRUE)
```

We can then clean this up to only be focused on the countries where this species occurs:

```{r}
# Intersect this with our country boundary to remove areas in the ocean
aoi <- terra::intersect(ca_protea, sern_a_dissolve)
plot(ca_protea)
plot(aoi, col = 'red', add = TRUE)
plot(protea, add = TRUE)
```

#### Process the covariate data

There are a few steps we need to take to clean up our worldclim data. First we crop and mask the covariates to our area of interest. 

```{r}
# Mask the covariates to the area of interest
covariates <- mask(crop(r, aoi), aoi)
plot(covariates[[1]])
plot(aoi, add = TRUE)
```

We then rename the bands to more sensible, readable names.

```{r}
# rename the worldclim bands to more reasonable names
names(covariates)
names(covariates) <- c("mean_ann_t","mean_diurnal_t_range","isothermality", "t_seas", 'max_t_warm_m','min_t_cold_m',"t_ann_range",'mean_t_wet_q','mean_t_dry_q','mean_t_warm_q','mean_t_cold_q','ann_p', 'p_wet_m','p_dry_m','p_seas','p_wet_q','p_dry_q','p_warm_q','p_cold_q')
names(covariates)
```

Rasters are often stored with a scaling factor to save on file size. For the temperature data, we will need to rescale these back to their original values.

```{r}
# Re-scale temperature values
covariates[[c(1:2,5:11)]] <- covariates[[c(1:2,5:11)]]/10
covariates[[3:4]] <- covariates[[3:4]]/100
```

#### Check for colinearity

Of these 19 different bioclimatic variables, several of them provide us with very similar, redundant information. To avoid this, we can check for collinearity above a certain threshold (0.7 is commonly used).

```{r}
# Using Pearson correlation
cov_colin <- correct_colinvar(covariates, method = c('pearson', th = "0.7"))
# Take a look at the correlations using corrplot
pdf('output/figs/SDM/corr_plot.pdf', width = 8, height = 6)
corrplot(cov_colin$cor_table, tl.cex = 0.6)
dev.off()
```

```{r}
# Show which variables are correlated
cov_colin$cor_variables
```

We can then select a subset of variables that we will use going forward. Don't forget though, that the other removed variables that are correlated to our selected variables could also likely play an important role in determining the suitability range of our species.

```{r}
# Select the subset we want
selected_vars <- c('min_t_cold_m', 'max_t_warm_m','isothermality','ann_p','p_seas')

# Subset the covariate data
cov_clean <- covariates[[selected_vars]]
cov_clean
```

#### Presence filtering

We can now work on our presence data. To avoid areas that may be over-sampled with highly clustered points that may create bias in our model output, we can run a **thinning** function. In this case, we will use a function `flexSDM::occfilt_env()` to avoid points with overly similar environmental covariate values. This will remove several locality records, but leave us with better data overall.

```{r}
# select only the lon/lat and provide a unique ID to each row
protea_df <- as.data.frame(protea) %>% select(lon, lat)
protea_df$id <- 1:nrow(protea_df)

# Run a filter on presence values based on the similarity in values of the environmental covariates
occ_filt_10bin <- occfilt_env(
  data = protea_df,
  x = 'lon',
  y = 'lat',
  id = 'id',
  env_layer = cov_clean,
  nbins = 10 # run ?occfilt_env to find out more on the method
)
# This removes ~120 records and only keeps ~100 records due to similarity!

# Plot our old points and new filtered points
par(mfrow = c(1,1))
plot(cov_clean[[1]]); 
points(protea_df, pch = 19, cex = 0.3);
points(occ_filt_10bin[,2:3], pch = 19, cex = 0.3, col = 'red')
# most of the filtering is happening where the points are spatially clustered...
```

Save these to a new data frame and then assign a value to indicate these are presence points (i.e. 1).

```{r}
# Save the 10 bin environmental filtering
protea_filt_pres <- occ_filt_10bin[,2:3]

# Assign 1 to represent presence
protea_filt_pres$pr_ab <- 1
```

#### Spatial block cross-validation

We now want to partition our data into different spatial blocks or "folds" to avoid spatial autocorrelation. This set of function will help to create spatially similar groups. When we later run our models, we will then be able to see if our models can predict to areas with spatially dissimilar values. This is an important step in any model that has a spatial element to it.

First, run the `blockCV::spatialAutoRange()` to find what the recommended size of our spatial blocks should be. This is based on the similarity in values across each environmental covariate. 

```{r}
# find the range value of block sizes by fitting variograms to each environmental raster to find the effective range of spatial autocorrelation
spat_range <- spatialAutoRange(
  rasterLayer = raster::raster(cov_clean),
  speciesData = st_as_sf(protea_filt_pres, coords = c('lon','lat'), crs = crs(cov_clean)),
  doParallel = TRUE,
  showPlots = TRUE)
# This suggest ~120km as an appropriate size for our blocks
```

This suggests a value of approximately 120 km as an appropriate size for our spatial blocks. We can now assign these blocks to k different spatial folds. In this case, we chose k = 4 folds as we have a limited amount of locality points. 

```{r}
# We can now create our spatial folds (k) for later cross-validation
# set the random seed, so this the output is the same across machines
k = 4

spat_blocks1 <- spatialBlock(
  speciesData = st_as_sf(protea_filt_pres, coords = c('lon','lat'), crs = crs(cov_clean)),
  species = "pr_ab",
  rasterLayer = raster::raster(cov_clean),
  k = k,
  theRange = spat_range$range,
  border = st_as_sf(aoi),
  seed = 101
)
```

```{r}
ggsave('output/figs/SDM/spatial_block.png',
       width = 6, height = 5, dpi = 'retina', bg = 'white')
```

We can now assign the fold IDs back onto our species presence data frame. 

```{r}
# Assign the folds (or partitions) back onto the presence dataset
protea_filt_pres$folds <- spat_blocks1$foldID
# Each point is now assigned to 1 of 4 partitions
head(protea_filt_pres)
```

Check to see how many presences are found within each spatial fold:

```{r}
# count the number of presences in each fold
protea_filt_pres %>% group_by(folds) %>% count()
```

Conver the spatial blocks to a raster, so that we can later extract this for our pseudo-absence data.

```{r}
# Rasterize the blocks
grid_env <- rasterize(vect(spat_blocks1$blocks), cov_clean, field = 'folds')
plot(grid_env)
```

#### Create pseudo-absences

The creating of pseudo-absence or background points is a crucial step in many SDMs. Most SDMs require both presence (1) and absence (0) data to properly evaluate the model accuracy. However, most species data is presence-only. The way around this is to create artifical points, which could replicate the effect of having absence points. These are called **pseudo-absences**. There is a lot of debate on the best approach for the producing these pseudo-absences as they can have a major impact on our overall model predictions and evaluation. Read more about this [here](https://www.biorxiv.org/content/10.1101/2022.03.24.485693v1.full).

Pseudo-absences can be create randomly, in a equal or proportionally-stratified manner or several other ways. Here we will use a proportionally-stratified approach and make sure that each spatial fold has the same number of both presences and absences by looping over each iteration of our spatial folds (1:k). 

Additionally, we impose a buffer on our pseudo-absence points ot make sure they are at least 20 km away from our presence points. We could alternatively use an environmental buffer, where the pseudo-absence points are placed in sites with a strong environmental dissimilarity from our presence points. Try out the alternative approach in your own time. 

```{r}
# We can now create our pseudo-absences and make sure they are evenly spaced across each of our fold grids, based on the number of presence points we have in each fold
# We can also make sure that they are placed away from our presence points by a environmental or distance buffer - here we use a 20 km distance buffer
# pseudo-absences
pa <- lapply(1:k, function(x) {
  sample_pseudoabs(
  data = protea_filt_pres,
  x = 'lon',
  y = 'lat',
  n = sum(protea_filt_pres$folds == x),
  # method = c('env_const', env = cov_clean), # constrain to env less suitable places based on bioclim model of protea presences
  # method = 'random',
  method = c('geo_const', width = 20000),
  maskval = x,
  rlayer = grid_env,
  calibarea = aoi
)
}) %>% bind_rows() 
```

We then extract the value of each fold for our pseudo-absence points using our rasterized spatial folds. 

```{r}
# Extract the partition number for each pseudo-absence point
pa <- sdm_extract(data = pa, x = "lon", y = "lat", env_layer = grid_env)
head(pa)
```

Check to see if we have the same number of presence and pseudo-absence points in each fold:

```{r}
# Count the number of pseudo-absence & presence points in each fold and see if they are equal
pa %>% group_by(folds) %>% count() == protea_filt_pres %>% group_by(folds) %>% count()
```

We can now plot this out to see what it looks like in space. Notice that the environmentally similar blocks may be found on opposite sides of our area of interest. This is likely due to the simple nature of our environmental covariates. If we included different coviarates, we would get a very different spatial block pattern.

```{r}
#### Let's plot the presences and pseudo-absences and view which folds they fall into
ggplot() +
  geom_sf(data = st_as_sf(aoi), fill = NA) +
  geom_sf(data = st_as_sf(spat_blocks1$blocks)) +
  geom_point(data = rbind(protea_filt_pres, pa), aes(x = lon, y = lat, col = as.factor(folds), pch = as.factor(pr_ab))) +
  labs(colour = 'Folds', shape = 'Presence/\nPseudo-absence') +
  theme_void()
```

```{r}
ggsave('output/figs/SDM/folds_and_points.png',
       width = 6, height = 5, dpi = 'retina', bg = 'white')
```

#### Extract covariate values for each point

In most SDM workflows, we could now extract the covariate data for each point using a function like `terra::extract()`. To see this in action, take a look at Tutorial 1 in the Extras Tab. However, the `SDMtune` package has specialised data structure called SWD (Sample with Data), which we will make use of. This is designed to specifically work together with the `train()` function that we will use when building our random forest.

```{r}
# Prepare a SWD (Sample with Data), which is a class of data specifically used in the SDMtune package
SWDdata <- prepareSWD(
  species = 'Protea roupelliae',
  p = protea_filt_pres[,1:2],
  a = pa[,1:2],
  env = cov_clean
)

# For some reason ID is included, so we need to remove this...
SWDdata@data <- SWDdata@data[-1]

head(SWDdata@data)
```

#### Fit the model

We have now thoroughly processed our species and covariate data and we are finally ready to fit our models.

We will run two different types of models. First one using random folds (i.e. no spatial component) and secondly one based on our spatial folds. 

For both sets of models, we will use the default settings for a random forest. Many modeling exercises may suggest hyper-parameter tuning, which means trying out several iterations of the settings until finding the best settings for our data. Read up more on how to implement this in [SDMtune](https://consbiol-unibern.github.io/SDMtune/articles/articles/tune-hyperparameters.html).

#### RandomForest with random folds

```{r}
# We can now fit our models! Let's first create random folds so that we can compare the output with the spatial blocking folds
rand_folds <- randomFolds(SWDdata, k = 4, only_presence = TRUE, seed = 1)

# Run a RandomForest model with default setting and random folds
set.seed(1)
rf_randcv <- train(method = 'RF', data = SWDdata, folds = rand_folds)
```

```{r}
# Check the overall AUC and TSS values
paste0('Testing AUC: ', round(SDMtune::auc(rf_randcv, test = TRUE),2))
paste0('Testing TSS: ', round(SDMtune::tss(rf_randcv, test = TRUE),2))
```

##### RandomForest with spatial folds

```{r}
# Our initial spatial blocking did not include the pseudo-absence data, so let's re-run the spatialBlock function to include both presence (1) and pseudo-absences (0)
# we can now use our previously predefined blocks
spat_blocks2 <- spatialBlock(speciesData = st_as_sf(bind_rows(protea_filt_pres, pa), coords = c('lon','lat'), crs = crs(cov_clean)), 
                   species = "pr_ab",
                   rasterLayer = raster::raster(cov_clean)[[1]], 
                   selection = 'predefined',
                   k = k,
                   blocks = spat_blocks1$blocks,
                   foldsCol = "folds",
                   seed = 101) 
# View the output
spat_blocks2$folds
```

As we can see, we have 4 folds In each of these we have our training data [[1]] and our testing data [[2]]. Also, as you can see from the console output, the training/testing split is approximately 70% training, 30% testing.

```{r}
# Run a RandomForest model with default setting and spatial folds
set.seed(1)
rf_sbcv <- train(method = 'RF', data = SWDdata, folds = spat_blocks2)
```

```{r}
# Check the overall AUC and TSS values
paste0('Testing AUC: ', round(SDMtune::auc(rf_sbcv, test = TRUE),2))
paste0('Testing TSS: ', round(SDMtune::tss(rf_sbcv, test = TRUE),2))
```

```{r}
# Extract the ROC curve and AUC values for each model
source('scripts/functions/extract_roc_vals.R')
spec_sens_vals <- extract_spec_sens_vals(rf_sbcv, spat_blocks2, SWDdata)
auc_vals <- extract_auc_vals(rf_sbcv, spat_blocks2, SWDdata)
auc_vals$label <- paste0(auc_vals$model_no, ": ", round(auc_vals$auc,2))
```

```{r}
# ROC curves with AUC values for each model 
ggplot(data = spec_sens_vals) +
  geom_abline(aes(slope = 1, intercept = 0), lty = 2) +
  geom_path(aes(x = 1- specificities, y = sensitivities, group = model_no, col = as.factor(model_no)), alpha = 0.8) +
  scale_colour_viridis_d(name = 'Model no. & AUC',
                         labels = auc_vals$label) +
  labs(x = 'False Positive Rate', y = 'True Positive Rate') +
  geom_text(aes(x = 0.15, y = 0.95), label = paste0('Overall testing AUC: ', round(SDMtune::auc(rf_sbcv, test = TRUE),2)), size = 3) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        legend.position = c(0.8, 0.25),
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 7))
```

```{r}
ggsave('output/figs/SDM/rf_sbcv_auc_plot.png',
       width = 6, height = 5, dpi = 'retina', bg = 'white')
```

#### Variable importance

```{r}
# VI for the random fold RF model
vi_rf_randcv <- varImp(rf_randcv)
plotVarImp(vi_rf_randcv)
```

```{r}
# VI for the spatial fold RF model
vi_rf_sbcv <- varImp(rf_sbcv)
plotVarImp(vi_rf_sbcv)
```

```{r}
ggsave('output/figs/SDM/rf_sbcv_vi.png',
       width = 6, height = 5, dpi = 'retina', bg = 'white')
```

#### Response curves

```{r}
plotResponse(rf_sbcv, var = "ann_p", marginal = TRUE, rug = TRUE) + labs(x = 'Ann. precip.') +
plotResponse(rf_sbcv, var = "max_t_warm_m", marginal = TRUE, rug = TRUE) + labs(x = 'Max. temp. warmest month')
```

```{r}
ggsave('output/figs/SDM/rf_sbcv_response_curves.png',
       width = 8, height = 4, dpi = 'retina', bg = 'white')
```

#### Model prediction

We can now predict which areas appear most suitable to our target species across our full area of interest using  predict() and our environmental layers.

```{r}
pred <- predict(rf_sbcv, data = raster::stack(cov_clean))
```

```{r}
# using the SMDtune::plotPred can give us a nice quick map
plotPred(pred, lt = "Habitat\nsuitability", colorramp = c("#2c7bb6", "#abd9e9", "#ffffbf", "#fdae61", "#d7191c"))
```

```{r}
# We can also customise this further in ggplot if we like
pred_df <- as.data.frame(pred, xy = TRUE)

ggplot() +
  geom_sf(data = st_as_sf(sern_a), fill = 'white', col = NA) + 
  geom_tile(data = pred_df, aes(x = x, y = y, fill = layer, col = layer)) +
  scale_colour_viridis_c(na.value = NA, option = 'C', breaks = seq(0,1,0.25),limits = c(0,1)) +
  scale_fill_viridis_c(na.value = NA, option = 'C', breaks = seq(0,1,0.25),limits = c(0,1)) +
  geom_sf(data = st_as_sf(sern_a), fill = NA, col = 'black', lwd = 0.25) + 
  geom_sf(data = st_as_sf(protea_filt_pres, coords = c('lon', 'lat'), crs = crs(sern_a)), size = 0.5, col = 'black', fill = 'white', pch = 21) +
  scale_x_continuous(limits = c(ext(cov_clean)[1], ext(cov_clean)[2]), breaks = seq(26,32,3)) + 
  scale_y_continuous(limits = c(ext(cov_clean)[3],ext(cov_clean)[4]), breaks = seq(-32,-22,5)) +
  labs(fill = 'Habitat\nsuitability', 
       col = 'Habitat\nsuitability',
       x = 'Longitude', y = 'Latitude') +
  theme_minimal() 
```

```{r}
ggsave('output/figs/SDM/rf_sbcv_hab_suit.png',
       width = 6, height = 6, dpi = 'retina', bg = 'white')
```

